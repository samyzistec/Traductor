{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer desde cero para Traducción Náhuatl (ncx) ↔ Español (es)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Documentación robusta — Transformer Nahuatl (ncx) ↔ Español (es)\n\n**Proyecto:** *articulo traduccion Nahualt-español*  \n**Implementación principal:** Transformer “from scratch” en PyTorch + baseline opcional **BERT2BERT (mBERT)**.  \n**Destino:** Notebook único: `Transformer_Nahuatl_Espanol_FromScratch.ipynb`\n\n### 1) Objetivo y alcance\nConstruir un sistema de traducción automática **bidireccional** `ncx ↔ es` con:\n\n- **Preprocesamiento específico:** segmentación por oraciones, normalización a minúsculas y preservación de **diacríticos, glotal, puntuación y números**.  \n- **Tokenización:** SentencePiece **Unigram** con **vocabulario compartido** (ncx+es) ≈ **10k**.  \n- **Modelo desde cero:** Transformer **Pre-Norm**, `d_model=512`, `n_heads=8`, `d_ff=2048`, `n_layers=6/6`, `dropout=0.1`.  \n- **Entrenamiento:** Adam + **Noam**, **label smoothing=0.1**, **bucketing** por longitud, **grad clip=1.0**.  \n- **Evaluación:** *sacreBLEU* y **chrF++**.  \n- **Inferencia:** *greedy* y **beam search** (beam configurable).  \n- **UI:** **Gradio** embebida en el notebook.  \n- **Baseline opcional:** **BERT2BERT (mBERT)** con HuggingFace para comparación.\n\n### 2) Requisitos\n**Software**\n- Python 3.9–3.11  \n- Paquetes mínimos: `sentencepiece`, `sacrebleu`, `gradio`, `tqdm`, `pyyaml`, `torch`  \n- Opcionales: `transformers`, `accelerate`, `datasets` (baseline), `torch-directml` (Windows/AMD), `spacy` + `es_core_news_sm`\n\n**Instalación (celda 0)**\n```bash\n%pip install sentencepiece sacrebleu gradio tqdm pyyaml\n%pip install transformers accelerate datasets  # baseline opcional\n%pip install torch-directml                    # opcional AMD/DirectML\n# %pip install spacy && python -m spacy download es_core_news_sm  # opcional\n```\n\n**Hardware**\n- Funciona en **CPU**. Si tienes **GPU AMD** en Windows, prueba **DirectML** (`torch-directml`). El notebook lo detecta automáticamente.\n\n### 3) Datos\n**Carpeta base:** `C:\\\\Users\\\\Samuel Perez\\\\Desktop\\\\articulo`\n\nEstructura esperada:\n```\narticulo/\n├─ salida/\n│  └─ parallel_ncx_es.jsonl       # corpus paralelo (requerido)\n├─ spm/                           # modelos SentencePiece\n├─ checkpoints/                   # pesos del modelo\n└─ logs/\n```\n\n**Formato del corpus (`parallel_ncx_es.jsonl`)**\nCada línea es un JSON como:\n```json\n{\"src\": \"<texto nahuatl>\", \"tgt\": \"<texto español>\", \"libro\":\"Génesis\", \"capitulo\":1, \"versiculo\":\"1\"}\n```\nLos campos `libro`, `capitulo`, `versiculo` son opcionales (trazabilidad).\n\nSi alguna vez necesitas **CSV→JSONL**:\n```python\nimport csv, json, pathlib\ncsv_path = pathlib.Path(r\"C:\\Users\\Samuel Perez\\Desktop\\articulo\\corpus_ncx_es.csv\")\njsonl_path = pathlib.Path(r\"C:\\Users\\Samuel Perez\\Desktop\\articulo\\salida\\parallel_ncx_es.jsonl\")\nwith open(csv_path, newline='', encoding='utf-8') as f, open(jsonl_path, 'w', encoding='utf-8') as w:\n    for row in csv.DictReader(f):\n        obj = {\"src\": row[\"ncx\"].strip(), \"tgt\": row[\"es\"].strip()}\n        w.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n```\n\n**Normalización y segmentación**\n- Minúsculas en ambos lados (preservando diacríticos y glotal).  \n- **Español:** spaCy si está disponible, sino regex.  \n- **Náhuatl:** reglas por puntuación.  \n- Si el nº de oraciones coincide en ambos lados, se divide verso→oración; si no, se conserva el verso.\n\n> Sugerencias: deduplicar pares, filtrar longitudes extremas, vigilar UTF‑8/ZWSP.\n\n### 4) Tokenización — SentencePiece (Unigram, vocab compartido)\n- `VOCAB_SIZE = 10000` (rango 8k–12k).  \n- Símbolos especiales: `<pad>`, `<bos>`, `<eos>`, `<lang_ncx>`, `<lang_es>`.  \n- `character_coverage = 1.0` para preservar diacríticos.  \n- Salida: `spm/ncx_es_unigram.model`, `spm/ncx_es_unigram.vocab`.\n\n### 5) Modelo “from scratch” (PyTorch)\n**Small (por defecto):** `d_model=512, n_heads=8, d_ff=2048, n_layers=6/6, dropout=0.1`  \n**Light (memoria baja):** `d_model=256, n_heads=4, d_ff=1024, n_layers=4/4`  \n- Pre‑Norm, positional encoding seno/coseno, **padding mask** y **causal mask** en decoder.\n\n### 6) Entrenamiento\n- `MAX_LEN=128`, **bucketing** por longitud.  \n- Adam(β1=0.9, β2=0.98, eps=1e-9) + **Noam** (`warmup=4000`).  \n- Regularización: **label smoothing=0.1**, **dropout=0.1**, **grad_clip=1.0**.  \n- **Grad Accum**: 1 (sube a 2–4 si reduces batch).  \n- Recomendación inicial (**salida recomendada**): `CFG_SMALL`, `EPOCHS=2`, `BATCH=32`, `WARMUP=4000`, `BEAM=5`, `length_penalty=0.7`.\n\n### 7) Evaluación\n- **sacreBLEU** y **chrF++** por dirección (`ncx→es` y `es→ncx`).  \n- En bajo‑recursos, **chrF++** suele ser más sensible.\n\n### 8) Inferencia y UI\n- *Greedy* (rápido) y **beam search** (beam 4–8 recomendado).  \n- **Gradio**: selector de dirección, slider de beam y cajas de entrada/salida. Ejecuta `demo.launch()`.\n\n### 9) Baseline opcional — BERT2BERT (mBERT)\n- `EncoderDecoderModel` con `bert-base-multilingual-cased` (freezing parcial).  \n- Dataset con `datasets` y `BertTokenizerFast`. Útil como comparación.\n\n### 10) Flujo de trabajo (paso a paso)\n1. Verifica `parallel_ncx_es.jsonl` en `...\\articulo\\salida\\`.  \n2. Corre celdas 0–7 (deps, rutas, carga/segmentación, splits, SentencePiece).  \n3. Revisa `VOCAB_SIZE=10k` y `MAX_LEN=128`.  \n4. Entrena (celda 11) ambas direcciones (empieza con 2 épocas).  \n5. Evalúa (celda 12) con BLEU/chrF++.  \n6. UI (celda 13): `demo.launch()` y prueba.  \n7. (Opcional) Baseline mBERT (celdas 14.x).\n\n### 11) Configuración avanzada (resumen)\n- Filtros por longitud, detección de idioma opcional, re‑segmentación conservadora.  \n- Decoding: beam 4–8, length penalty 0.6–1.0, penalización de repetición si hay loops.  \n- Rendimiento: usa `CFG_LIGHT`, baja `BATCH`, sube `grad_accum`.  \n- Reanudación: utilidades de `resume_direction(...)` incluidas.\n\n### 12) Estructura de salida\n- `spm/*.model`, `spm/*.vocab`  \n- `checkpoints/scratch_ncx2es_best.pt`, `checkpoints/scratch_es2ncx_best.pt`  \n- `hf_bert2bert/*` (opcional)\n\n### 13) FAQ rápida\n1. **AssertionError** (no está `parallel_ncx_es.jsonl`) → confirma ruta; si es CSV, convierte con el snippet.  \n2. **unicodeescape** en rutas Windows → usa raw strings `r\"C:\\...\"` o `/`.  \n3. Falta `sentencepiece/sacrebleu/gradio` → instala en celda 0.  \n4. Memoria insuficiente → `CFG_LIGHT`, baja `BATCH`, sube `grad_accum`, reduce `MAX_LEN`.  \n5. Gradio no abre → `demo.launch(share=False)` y navega a `127.0.0.1`.  \n6. DirectML no se usa → instala `torch-directml`; si no, continúa en CPU.\n\n### 14) Consideraciones lingüísticas\n- Preservar diacríticos/glotal.  \n- Morfología rica: **Unigram** suele fragmentar mejor que BPE.  \n- Segmentación por puntuación es conservadora; se puede entrenar un sentencizer específico más adelante.\n\n### 15) Ética y licencias\nRespeta términos de uso de las fuentes (p. ej., JW.org), documenta limitaciones y evita usos sensibles sin revisión humana.\n\n### 16) Roadmap sugerido\n- **v0.1 (actual):** pipeline completo + UI + baseline mBERT.  \n- **v0.2:** métricas automáticas por época (chrF++ en dev), early stopping y CSV/JSON.  \n- **v0.3:** mBART‑50/mT5, limpieza/alineación, análisis de errores.  \n- **v0.4:** post‑edición asistida y glosario.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Proyecto: **articulo traduccion Nahualt-español** — Notebook unificado con preprocesamiento, tokenización, modelo Transformer, entrenamiento, evaluación, inferencia/UI y baseline mBERT.  \n> **Stack:** PyTorch puro + SentencePiece + sacreBLEU/chrF++ + Gradio.  \n> **Ruta base**: `C:\\Users\\Samuel Perez\\Desktop\\articulo` (ajústala si lo necesitas).\n\n### Índice\n1. Objetivo y requisitos  \n2. Datos y segmentación  \n3. Tokenización (SentencePiece)  \n4. Modelo Transformer (Pre‑Norm)  \n5. Entrenamiento (Noam, label smoothing, bucketing)  \n6. Logging CSV + Early stopping (chrF++) + reanudar  \n7. Evaluación (BLEU/chrF++)  \n8. Inferencia (greedy/beam) + UI con Gradio  \n9. Baseline opcional BERT2BERT (mBERT)\n\n---\n\n## 1) Objetivo\n- NMT **bidireccional** ncx↔es con vocab **compartido**.  \n- Respetar diacríticos/glotal y signos de puntuación, minúsculas en ambos lados.\n\n## 2) Requisitos\nInstala si hace falta:\n```bash\n%pip install sentencepiece sacrebleu gradio tqdm pyyaml\n# Opcional (baseline mBERT):\n%pip install transformers accelerate datasets\n# Opcional (Windows + AMD/DirectML):\n%pip install torch-directml\n# Opcional (mejor segmentación español):\n# %pip install spacy && python -m spacy download es_core_news_sm\n```\n\n## 3) Datos\nSe requiere `salida/parallel_ncx_es.jsonl` con líneas tipo:\n```json\n{\"src\":\"<nahuatl>\", \"tgt\":\"<español>\", \"libro\":\"Génesis\", \"capitulo\":1, \"versiculo\":\"1\"}\n```\nSi el número de oraciones coincide (ncx y es), el verso se divide en múltiples pares; de lo contrario se deja tal cual.\n\n## 4) Arquitectura y entrenamiento\n- **Pre‑Norm**, `d_model=512`, `n_heads=8`, `ff=2048`, `n_layers=6/6`, `dropout=0.1`.  \n- **Adam + Noam (warmup=4000)**, **label smoothing=0.1**, **grad_clip=1.0**.  \n- **MAX_LEN=128**, **bucketing** por longitud.  \n- **Early stopping** por **chrF++**; logging CSV por época en `logs/`.\n\n> Consejos: empieza con `EPOCHS=2` para validar; sube a 6–10 si todo va bien. Usa `CFG_LIGHT` si hay poca RAM.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 1) Rutas, semillas y splits\nfrom pathlib import Path\nimport os, random, numpy as np, torch\n\nBASE_DIR = Path(r\"C:\\Users\\Samuel Perez\\Desktop\\articulo\")\nfor p in [BASE_DIR, BASE_DIR/\"salida\", BASE_DIR/\"checkpoints\", BASE_DIR/\"spm\", BASE_DIR/\"logs\"]:\n    p.mkdir(parents=True, exist_ok=True)\n\nDATA_DIR = BASE_DIR / \"salida\"\nCHECK_DIR = BASE_DIR / \"checkpoints\"\nTOK_DIR = BASE_DIR / \"spm\"\nLOG_DIR = BASE_DIR / \"logs\"\nPARALLEL_JSONL = DATA_DIR / \"parallel_ncx_es.jsonl\"\nassert PARALLEL_JSONL.exists(), f\"No se encontró {PARALLEL_JSONL}. Coloca el corpus en esa ruta.\"\n\nSEED=42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\nSPLIT_TRAIN=0.8; SPLIT_DEV=0.1\nMAX_SAMPLES=0\nMAX_LEN=128\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 2) Dispositivo\nimport torch\nDEVICE = torch.device(\"cpu\")\ntry:\n    import torch_directml\n    DEVICE = torch_directml.device()\n    print(\"Usando DirectML (GPU AMD) si está disponible.\")\nexcept Exception as e:\n    print(\"DirectML no disponible; usando CPU.\\n\", str(e))\nprint(\"DEVICE =\", DEVICE)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 3) Carga de datos y segmentación\nimport json, re\nfrom typing import List\ndef sent_split_es(text: str) -> List[str]:\n    try:\n        import spacy\n        try: nlp = spacy.load(\"es_core_news_sm\")\n        except Exception:\n            nlp = spacy.blank(\"es\"); nlp.add_pipe(\"sentencizer\")\n        return [s.text.strip() for s in nlp(text).sents if s.text.strip()]\n    except Exception:\n        parts = re.split(r\"(?<=[\\.\\?\\!¡¿])\\s+\", text.strip())\n        return [p.strip() for p in parts if p.strip()]\ndef sent_split_ncx(text: str) -> List[str]:\n    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text.strip())\n    return [p.strip() for p in parts if p.strip()]\n\npairs = []\nwith open(PARALLEL_JSONL, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        obj = json.loads(line)\n        s = obj[\"src\"].strip(); t = obj[\"tgt\"].strip()\n        if s and t: pairs.append((s, t, obj.get(\"libro\",\"\"), obj.get(\"capitulo\",0), obj.get(\"versiculo\",0)))\nprint(f\"Pares cargados (verso): {len(pairs):,}\")\n\nexpanded = []\nfor s, t, libro, cap, ver in pairs:\n    ss = sent_split_ncx(s.lower()); tt = sent_split_es(t.lower())\n    if 1 < len(ss) == len(tt) < 10:\n        for i in range(len(ss)): expanded.append((ss[i], tt[i], libro, cap, f\"{ver}.{i+1}\"))\n    else:\n        expanded.append((s.lower(), t.lower(), libro, cap, ver))\nif MAX_SAMPLES and MAX_SAMPLES>0: expanded = expanded[:MAX_SAMPLES]\nprint(f\"Pares tras segmentación: {len(expanded):,}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 4) Splits\nfrom math import floor\nidx = list(range(len(expanded))); random.shuffle(idx)\nn_tr = floor(len(idx)*SPLIT_TRAIN); n_de = floor(len(idx)*SPLIT_DEV)\ndef take(idxs): return [expanded[i] for i in idxs]\ntrain_pairs = take(idx[:n_tr]); dev_pairs = take(idx[n_tr:n_tr+n_de]); test_pairs = take(idx[n_tr+n_de:])\nprint(f\"Train={len(train_pairs):,} | Dev={len(dev_pairs):,} | Test={len(test_pairs):,}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 5) SentencePiece\nimport sentencepiece as spm\nVOCAB_SIZE = 10000\nraw_corpus = (TOK_DIR / \"spm_raw.txt\")\nwith open(raw_corpus, \"w\", encoding=\"utf-8\") as w:\n    for s, t, *_ in train_pairs + dev_pairs: w.write(s+\"\\n\"); w.write(t+\"\\n\")\nSPM_MODEL_PREFIX = str((TOK_DIR / \"ncx_es_unigram\").as_posix())\nspm.SentencePieceTrainer.Train(\n    input=str(raw_corpus), model_prefix=SPM_MODEL_PREFIX, vocab_size=VOCAB_SIZE, model_type=\"unigram\",\n    user_defined_symbols=[\"<pad>\",\"<bos>\",\"<eos>\",\"<lang_ncx>\",\"<lang_es>\"], character_coverage=1.0,\n    input_sentence_size=1000000, shuffle_input_sentence=True)\nSPM_MODEL = TOK_DIR / \"ncx_es_unigram.model\"; SPM_VOCAB = TOK_DIR / \"ncx_es_unigram.vocab\"\nassert SPM_MODEL.exists(), \"No se generó el modelo SentencePiece.\"\nprint(\"Tokenizador listo:\", SPM_MODEL)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 6) Tokenización helpers\nsp = spm.SentencePieceProcessor(model_file=str(SPM_MODEL))\nPAD_ID=sp.piece_to_id(\"<pad>\"); BOS_ID=sp.piece_to_id(\"<bos>\"); EOS_ID=sp.piece_to_id(\"<eos>\")\nLNCX_ID=sp.piece_to_id(\"<lang_ncx>\"); LES_ID=sp.piece_to_id(\"<lang_es>\"); VOCAB=sp.get_piece_size()\ndef encode_with_lang(text, lang_tok_id):\n    ids = sp.encode(text, out_type=int); return [BOS_ID, lang_tok_id] + ids + [EOS_ID]\ndef collate_batch(batch, pad_id=PAD_ID):\n    src_lens=[len(b[0]) for b in batch]; tgt_lens=[len(b[1]) for b in batch]\n    max_src=min(max(src_lens), MAX_LEN); max_tgt=min(max(tgt_lens), MAX_LEN)\n    def pad_seq(seq,L): seq=seq[:L]; return seq+[pad_id]*(L-len(seq))\n    import torch\n    src=torch.tensor([pad_seq(b[0],max_src) for b in batch],dtype=torch.long)\n    tgt=torch.tensor([pad_seq(b[1],max_tgt) for b in batch],dtype=torch.long)\n    return src,tgt\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 7) Dataset + bucketing\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nclass ParallelDataset(Dataset):\n    def __init__(self, pairs, direction=\"ncx2es\"):\n        self.items=[]\n        for s,t,*_ in pairs:\n            if direction==\"ncx2es\": self.items.append((encode_with_lang(s,LNCX_ID), encode_with_lang(t,LES_ID)))\n            else: self.items.append((encode_with_lang(t,LES_ID), encode_with_lang(s,LNCX_ID)))\n    def __len__(self): return len(self.items)\n    def __getitem__(self,i): return self.items[i]\ndef make_loader(pairs, direction, batch_size=32, shuffle=True):\n    ds=ParallelDataset(pairs,direction=direction)\n    order=sorted(range(len(ds)),key=lambda i: len(ds.items[i][0]))\n    if shuffle:\n        B=50; buckets=[order[i::B] for i in range(B)]; order=[i for b in buckets for i in random.sample(b,len(b))]\n    class _P(Dataset):\n        def __len__(self): return len(order)\n        def __getitem__(self,j): return ds.items[order[j]]\n    return DataLoader(_P(), batch_size=batch_size, collate_fn=collate_batch)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 8) Modelo Transformer (Pre‑Norm)\nimport math, torch, torch.nn as nn\nDROPOUT = 0.1\nclass PositionalEncoding(nn.Module):\n    def __init__(self,d_model,max_len=2048):\n        super().__init__()\n        pe=torch.zeros(max_len,d_model); pos=torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n        div=torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model))\n        pe[:,0::2]=torch.sin(pos*div); pe[:,1::2]=torch.cos(pos*div); self.register_buffer('pe',pe.unsqueeze(0))\n    def forward(self,x): return x + self.pe[:, :x.size(1)]\nclass MultiHeadAttention(nn.Module):\n    def __init__(self,d_model,n_heads,dropout=0.1):\n        super().__init__(); assert d_model % n_heads == 0; self.d_k=d_model//n_heads; self.n=n_heads\n        self.q=nn.Linear(d_model,d_model); self.k=nn.Linear(d_model,d_model); self.v=nn.Linear(d_model,d_model); self.o=nn.Linear(d_model,d_model); self.drop=nn.Dropout(dropout)\n    def forward(self,q,k,v,attn_mask=None,key_padding_mask=None):\n        B,Lq,D=q.shape; B,Lk,_=k.shape\n        q=self.q(q).view(B,Lq,self.n,self.d_k).transpose(1,2); k=self.k(k).view(B,Lk,self.n,self.d_k).transpose(1,2); v=self.v(v).view(B,Lk,self.n,self.d_k).transpose(1,2)\n        scores=torch.matmul(q,k.transpose(-2,-1))/math.sqrt(self.d_k)\n        if attn_mask is not None:\n            if attn_mask.dim()==2: scores = scores + attn_mask.unsqueeze(0).unsqueeze(0)\n            elif attn_mask.dim()==4: scores = scores + attn_mask\n        if key_padding_mask is not None: scores=scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n        attn=torch.softmax(scores,dim=-1); attn=self.drop(attn); out=torch.matmul(attn,v).transpose(1,2).contiguous().view(B,Lq,D)\n        return self.o(out)\nclass FeedForward(nn.Module):\n    def __init__(self,d_model,d_ff,dropout=0.1): super().__init__(); self.net=nn.Sequential(nn.Linear(d_model,d_ff), nn.ReLU(), nn.Dropout(dropout), nn.Linear(d_ff,d_model))\n    def forward(self,x): return self.net(x)\nclass EncoderLayer(nn.Module):\n    def __init__(self,d_model,n_heads,d_ff,dropout=0.1):\n        super().__init__(); self.norm1=nn.LayerNorm(d_model); self.attn=MultiHeadAttention(d_model,n_heads,dropout); self.drop1=nn.Dropout(dropout); self.norm2=nn.LayerNorm(d_model); self.ff=FeedForward(d_model,d_ff,dropout); self.drop2=nn.Dropout(dropout)\n    def forward(self,x,src_pad_mask):\n        y=self.attn(self.norm1(x),self.norm1(x),self.norm1(x),key_padding_mask=src_pad_mask); x=x+self.drop1(y); y=self.ff(self.norm2(x)); x=x+self.drop2(y); return x\nclass DecoderLayer(nn.Module):\n    def __init__(self,d_model,n_heads,d_ff,dropout=0.1):\n        super().__init__(); self.norm1=nn.LayerNorm(d_model); self.self_attn=MultiHeadAttention(d_model,n_heads,dropout); self.drop1=nn.Dropout(dropout); self.norm2=nn.LayerNorm(d_model); self.cross_attn=MultiHeadAttention(d_model,n_heads,dropout); self.drop2=nn.Dropout(dropout); self.norm3=nn.LayerNorm(d_model); self.ff=FeedForward(d_model,d_ff,dropout); self.drop3=nn.Dropout(dropout)\n    def forward(self,x,mem,tgt_pad_mask,tgt_causal_mask,mem_pad_mask):\n        y=self.self_attn(self.norm1(x),self.norm1(x),self.norm1(x),attn_mask=tgt_causal_mask,key_padding_mask=tgt_pad_mask); x=x+self.drop1(y)\n        y=self.cross_attn(self.norm2(x),mem,mem,key_padding_mask=mem_pad_mask); x=x+self.drop2(y); y=self.ff(self.norm3(x)); x=x+self.drop3(y); return x\nclass TransformerModel(nn.Module):\n    def __init__(self,vocab_size,d_model,n_heads,d_ff,n_enc,n_dec,dropout=0.1,pad_id=0):\n        super().__init__(); self.pad_id=pad_id; self.emb=nn.Embedding(vocab_size,d_model,padding_idx=pad_id); self.pos=PositionalEncoding(d_model)\n        self.encoder=nn.ModuleList([EncoderLayer(d_model,n_heads,d_ff,dropout) for _ in range(n_enc)])\n        self.decoder=nn.ModuleList([DecoderLayer(d_model,n_heads,d_ff,dropout) for _ in range(n_dec)])\n        self.proj=nn.Linear(d_model,vocab_size)\n    def make_pad_mask(self,seq): return seq.eq(self.pad_id)\n    def make_causal_mask(self,L):\n        m=torch.triu(torch.ones(L,L,device=self.emb.weight.device),diagonal=1); return m.masked_fill(m==1,float('-inf'))\n    def encode(self,src):\n        src_pad=self.make_pad_mask(src); x=self.pos(self.emb(src))\n        for layer in self.encoder: x=layer(x,src_pad)\n        return x,src_pad\n    def decode(self,tgt,mem,mem_pad):\n        tgt_pad=self.make_pad_mask(tgt); x=self.pos(self.emb(tgt)); causal=self.make_causal_mask(tgt.size(1)).unsqueeze(0).unsqueeze(0)\n        for layer in self.decoder: x=layer(x,mem,tgt_pad,causal,mem_pad)\n        return self.proj(x)\n    def forward(self,src,tgt_in): mem,src_pad=self.encode(src); return self.decode(tgt_in,mem,src_pad)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 9) LabelSmoothing + Noam + métricas\nimport torch.nn as nn, torch\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self,classes,smoothing=0.1,ignore_index=0):\n        super().__init__(); self.ignore_index=ignore_index; self.confidence=1.0-smoothing; self.smoothing=smoothing; self.cls=classes\n    def forward(self,pred,target):\n        pred=pred.view(-1,pred.size(-1)); target=target.reshape(-1); log_probs=torch.log_softmax(pred,dim=-1)\n        nll=-log_probs.gather(dim=-1,index=target.unsqueeze(1)).squeeze(1); smooth=-log_probs.mean(dim=-1); pad_mask=target.eq(self.ignore_index)\n        loss=self.confidence*nll + self.smoothing*smooth\n        return (loss.masked_fill(pad_mask,0).sum()/torch.clamp((~pad_mask).sum(),min=1))\nclass NoamWrapper:\n    def __init__(self,opt,d_model,warmup=4000): self.opt=opt; self.d_model=d_model; self.warm=warmup; self.step_num=0\n    def step(self):\n        self.step_num+=1; lr=(self.d_model**-0.5)*min(self.step_num**-0.5, self.step_num*(self.warm**-1.5))\n        for pg in self.opt.param_groups: pg['lr']=lr\n        self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n    @property\n    def lr(self): return self.opt.param_groups[0]['lr']\ntry:\n    from sacrebleu.metrics import BLEU, CHRF\n    bleu=BLEU(force=True); chrf=CHRF(word_order=2)\nexcept Exception as e:\n    print(\"sacrebleu no disponible:\", e); bleu=chrf=None\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 10) Eval/greedy + CFGs\nimport torch\ndef batch_to_device(b,device): return b[0].to(device), b[1].to(device)\ndef evaluate(model,loader,device):\n    model.eval(); total=0.0; n=0\n    with torch.no_grad():\n        for src,tgt in loader:\n            src,tgt=batch_to_device((src,tgt),device)\n            logits=model(src,tgt[:,:-1])\n            crit=LabelSmoothingLoss(VOCAB,0.1,PAD_ID)\n            loss=crit(logits,tgt[:,1:]); total+=loss.item(); n+=1\n    return total/max(n,1)\ndef ids_to_text(ids):\n    ids=[i for i in ids if i not in (PAD_ID,BOS_ID)]\n    if ids and ids[-1]==EOS_ID: ids=ids[:-1]\n    return sp.decode(ids)\ndef translate_greedy(model, src_ids, max_len=MAX_LEN):\n    model.eval(); src=torch.tensor([src_ids],dtype=torch.long,device=model.emb.weight.device)\n    mem,src_pad=model.encode(src); ys=torch.tensor([[BOS_ID, LES_ID]],dtype=torch.long,device=src.device)\n    for _ in range(max_len):\n        logits=model.decode(ys,mem,src_pad); nxt=logits[:,-1,:].argmax(dim=-1,keepdim=True)\n        ys=torch.cat([ys,nxt],dim=1)\n        if nxt.item()==EOS_ID: break\n    return ys[0].tolist()\nCFG_SMALL=dict(d_model=512,n_heads=8,d_ff=2048,n_enc=6,n_dec=6)\nCFG_LIGHT=dict(d_model=256,n_heads=4,d_ff=1024,n_enc=4,n_dec=4)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 10b) Logging CSV + Early Stopping + Resume\nimport csv, pathlib\nfrom datetime import datetime\ndef ensure_logfile(direction):\n    LOG_DIR.mkdir(parents=True, exist_ok=True)\n    path = LOG_DIR / f\"train_log_{direction}.csv\"\n    if not path.exists():\n        with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as w:\n            csv.writer(w).writerow([\"timestamp\",\"direction\",\"epoch\",\"steps\",\"lr\",\"train_loss\",\"dev_loss\",\"dev_bleu\",\"dev_chrf\",\"best_so_far\"])\n    return path\ndef translate_greedy_dir(model, src_ids, direction=\"ncx2es\", max_len=MAX_LEN):\n    tgt_lang = LES_ID if direction==\"ncx2es\" else LNCX_ID\n    model.eval(); src=torch.tensor([src_ids],dtype=torch.long,device=model.emb.weight.device)\n    mem,src_pad=model.encode(src); ys=torch.tensor([[BOS_ID, tgt_lang]],dtype=torch.long,device=src.device)\n    for _ in range(max_len):\n        logits=model.decode(ys,mem,src_pad); nxt=logits[:,-1,:].argmax(dim=-1,keepdim=True); ys=torch.cat([ys,nxt],dim=1)\n        if nxt.item()==EOS_ID: break\n    return ys[0].tolist()\ndef compute_dev_metrics(model, direction=\"ncx2es\", max_samples=200):\n    ds=ParallelDataset(dev_pairs,direction=direction)\n    if len(ds)==0: return (0.0,0.0)\n    up=min(len(ds),max_samples); refs,hyps=[],[]\n    for i in range(up):\n        src_ids,tgt_ids=ds[i]; out_ids=translate_greedy_dir(model,src_ids,direction=direction,max_len=MAX_LEN)\n        refs.append([ids_to_text(tgt_ids)]); hyps.append(ids_to_text(out_ids))\n    try:\n        from sacrebleu.metrics import BLEU, CHRF\n        return (BLEU(force=True).corpus_score(hyps, list(zip(*refs))).score,\n                CHRF(word_order=2).corpus_score(hyps, list(zip(*refs))).score)\n    except Exception: return (0.0,0.0)\ndef save_checkpoint(model, save_prefix, d_model, n_heads, d_ff, n_enc, n_dec, pad_id, vocab, direction, epoch, noam_step=0):\n    CHECK_DIR.mkdir(parents=True, exist_ok=True)\n    path = (CHECK_DIR / f\"{save_prefix}_best.pt\").as_posix()\n    torch.save({\"model\":model.state_dict(),\n                \"cfg\":{\"d_model\":d_model,\"n_heads\":n_heads,\"d_ff\":d_ff,\"n_enc\":n_enc,\"n_dec\":n_dec,\"pad_id\":pad_id,\"vocab\":vocab},\n                \"meta\":{\"direction\":direction,\"epoch\":epoch,\"noam_step\":noam_step,\"spm_model\":str(SPM_MODEL)}},\n               path); return path\ndef train_direction(direction=\"ncx2es\", epochs=3, batch_size=32, grad_accum=1,\n                    d_model=512, n_heads=8, d_ff=2048, n_enc=6, n_dec=6,\n                    warmup=4000, save_prefix=\"scratch_ncx2es\",\n                    patience=3, dev_metric_samples=200, init_model=None, start_epoch=1):\n    print(f\"\\n=== Entrenando dirección: {direction} ===\")\n    train_loader=make_loader(train_pairs,direction,batch_size=batch_size,shuffle=True)\n    dev_loader=make_loader(dev_pairs,direction,batch_size=batch_size,shuffle=False)\n    model=TransformerModel(VOCAB,d_model,n_heads,d_ff,n_enc,n_dec,DROPOUT,PAD_ID) if init_model is None else init_model\n    model.to(DEVICE)\n    opt=torch.optim.Adam(model.parameters(),betas=(0.9,0.98),eps=1e-9); noam=NoamWrapper(opt,d_model,warmup=warmup); crit=LabelSmoothingLoss(VOCAB,0.1,PAD_ID)\n    log_path=ensure_logfile(direction); best_dev_chrf=-1e9; epochs_no_improve=0; best_path=None; global_step=0\n    for ep in range(start_epoch, start_epoch+epochs):\n        model.train(); total=0.0; n=0; opt.zero_grad()\n        for i,(src,tgt) in enumerate(train_loader,1):\n            src,tgt=src.to(DEVICE),tgt.to(DEVICE)\n            logits=model(src,tgt[:,:-1])\n            loss=crit(logits,tgt[:,1:])/grad_accum; loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n            if i % grad_accum == 0: noam.step(); noam.zero_grad()\n            total+=loss.item()*grad_accum; n+=1; global_step+=1\n            if i % 100 == 0: print(f\"ep{ep} step{i} lr={noam.lr:.6f} loss={total/max(n,1):.4f}\")\n        dev_loss=evaluate(model,dev_loader,DEVICE); dev_bleu,dev_chrf=compute_dev_metrics(model,direction=direction,max_samples=dev_metric_samples)\n        print(f\"[Ep {ep}] dev_loss={dev_loss:.4f} | BLEU={dev_bleu:.2f} | chrF++={dev_chrf:.2f}\")\n        with open(log_path,\"a\",encoding=\"utf-8\",newline=\"\") as w:\n            csv.writer(w).writerow([datetime.utcnow().isoformat(),direction,ep,global_step,f\"{noam.lr:.8f}\",f\"{total/max(n,1):.6f}\",f\"{dev_loss:.6f}\",f\"{dev_bleu:.4f}\",f\"{dev_chrf:.4f}\",\"yes\" if dev_chrf>best_dev_chrf else \"no\"])\n        if dev_chrf>best_dev_chrf:\n            best_dev_chrf=dev_chrf; best_path=save_checkpoint(model,save_prefix,d_model,n_heads,d_ff,n_enc,n_dec,PAD_ID,VOCAB,direction,ep,noam.step_num)\n            print(\"Mejora en chrF++; guardado mejor modelo en\", best_path); epochs_no_improve=0\n        else:\n            epochs_no_improve+=1; print(f\"Sin mejora de chrF++ por {epochs_no_improve}/{patience} épocas\")\n            if epochs_no_improve>=patience: print(\"Early stopping activado (paciencia agotada).\"); break\n    return best_path, best_dev_chrf\ndef load_for_resume(ckpt_path):\n    data=torch.load(ckpt_path,map_location=\"cpu\"); cfg=data[\"cfg\"]; meta=data.get(\"meta\",{})\n    model=TransformerModel(cfg[\"vocab\"],cfg[\"d_model\"],cfg[\"n_heads\"],cfg[\"d_ff\"],cfg[\"n_enc\"],cfg[\"n_dec\"],pad_id=cfg[\"pad_id\"])\n    model.load_state_dict(data[\"model\"]); model.to(DEVICE); return model,cfg,meta\ndef resume_direction(ckpt_path,more_epochs=2,batch_size=32,grad_accum=1,warmup=4000,patience=3,dev_metric_samples=200,save_prefix=None):\n    import pathlib\n    model,cfg,meta=load_for_resume(ckpt_path); direction=meta.get(\"direction\",\"ncx2es\")\n    if save_prefix is None: save_prefix=pathlib.Path(ckpt_path).stem + \"_cont\"\n    print(f\"Reanudando {direction} desde {ckpt_path}\")\n    return train_direction(direction=direction,epochs=more_epochs,batch_size=batch_size,grad_accum=grad_accum,\n                           d_model=cfg[\"d_model\"],n_heads=cfg[\"n_heads\"],d_ff=cfg[\"d_ff\"],n_enc=cfg[\"n_enc\"],n_dec=cfg[\"n_dec\"],\n                           warmup=warmup,save_prefix=save_prefix,patience=patience,dev_metric_samples=dev_metric_samples,\n                           init_model=model,start_epoch=meta.get(\"epoch\",1)+1)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 11) Entreno ambos sentidos (ajusta epochs/batch)\nEPOCHS=2; BATCH=32; ACCUM=1; WARMUP=4000\nbest_ncx2es,_ = train_direction(\"ncx2es\", epochs=EPOCHS, batch_size=BATCH, grad_accum=ACCUM, warmup=WARMUP, save_prefix=\"scratch_ncx2es\", **CFG_SMALL)\nbest_es2ncx,_ = train_direction(\"es2ncx\", epochs=EPOCHS, batch_size=BATCH, grad_accum=ACCUM, warmup=WARMUP, save_prefix=\"scratch_es2ncx\", **CFG_SMALL)\nprint(\"Mejores:\", best_ncx2es, best_es2ncx)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 12) Evaluación\nfrom pathlib import Path\ndef load_model(path):\n    data=torch.load(path,map_location=\"cpu\"); cfg=data[\"cfg\"]\n    model=TransformerModel(cfg[\"vocab\"],cfg[\"d_model\"],cfg[\"n_heads\"],cfg[\"d_ff\"],cfg[\"n_enc\"],cfg[\"n_dec\"],pad_id=cfg[\"pad_id\"])\n    model.load_state_dict(data[\"model\"]); model.to(DEVICE); model.eval(); return model\ndef eval_direction(best_path, direction=\"ncx2es\", max_samples=200):\n    if best_path is None or not Path(best_path).exists(): print(\"Checkpoint no encontrado:\", best_path); return\n    model=load_model(best_path); ds=ParallelDataset(test_pairs, direction=direction)\n    refs=[]; hyps=[]\n    for i in range(min(len(ds),max_samples)):\n        src_ids,tgt_ids=ds[i]; out_ids=translate_greedy(model, src_ids, max_len=MAX_LEN)\n        refs.append([ids_to_text(tgt_ids)]); hyps.append(ids_to_text(out_ids))\n    try:\n        from sacrebleu.metrics import BLEU, CHRF\n        print(direction,\"BLEU:\", BLEU(force=True).corpus_score(hyps, list(zip(*refs))))\n        print(direction,\"chrF++:\", CHRF(word_order=2).corpus_score(hyps, list(zip(*refs))))\n    except Exception as e: print(\"sacrebleu no disponible:\", e)\neval_direction(best_ncx2es,\"ncx2es\"); eval_direction(best_es2ncx,\"es2ncx\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 13) UI con Gradio\nimport gradio as gr, torch\ndef translate_beam(model, src_ids, beam=5, lp=0.7, max_len=MAX_LEN, tgt_lang_id=LES_ID):\n    model.eval(); device=model.emb.weight.device\n    src=torch.tensor([src_ids],dtype=torch.long,device=device); mem,src_pad=model.encode(src)\n    beams=[([BOS_ID,tgt_lang_id],0.0)]; finished=[]\n    for _ in range(max_len):\n        new=[]\n        for seq,sc in beams:\n            if seq[-1]==EOS_ID: finished.append((seq,sc)); continue\n            ys=torch.tensor([seq],dtype=torch.long,device=device)\n            logits=model.decode(ys,mem,src_pad)[:,-1,:].squeeze(0); logp=torch.log_softmax(logits,dim=-1).detach().cpu()\n            topk=torch.topk(logp,beam).indices.tolist()\n            for tok in topk: new.append((seq+[tok], sc+logp[tok].item()))\n        beams=sorted(new,key=lambda x: x[1]/((len(x[0])**lp)), reverse=True)[:beam]\n        if not beams: break\n    if not finished: finished=beams\n    best=max(finished,key=lambda x: x[1]/((len(x[0])**lp))); return best[0]\nBEST_NCX2ES=best_ncx2es if 'best_ncx2es' in globals() else None\nBEST_ES2NCX=best_es2ncx if 'best_es2ncx' in globals() else None\ndef _load_model_(path):\n    data=torch.load(path,map_location=\"cpu\"); cfg=data[\"cfg\"]\n    m=TransformerModel(cfg[\"vocab\"],cfg[\"d_model\"],cfg[\"n_heads\"],cfg[\"d_ff\"],cfg[\"n_enc\"],cfg[\"n_dec\"],pad_id=cfg[\"pad_id\"]); m.load_state_dict(data[\"model\"]); m.eval(); return m\ndef load_scratch(direction):\n    path = BEST_NCX2ES if direction==\"ncx2es\" else BEST_ES2NCX\n    from pathlib import Path\n    if path is None or not Path(path).exists(): return None, f\"Checkpoint no encontrado: {path}\"\n    return _load_model_(path), f\"Cargado: {path}\"\ndef infer_scratch(text, direction=\"ncx2es\", beam=5):\n    if not text.strip(): return \"\"\n    model, msg = load_scratch(direction)\n    lang_id = LES_ID if direction==\"ncx2es\" else LNCX_ID\n    src_lang = LNCX_ID if direction==\"ncx2es\" else LES_ID\n    src_ids = encode_with_lang(text.lower(), src_lang)\n    out_ids = translate_beam(model, src_ids, beam=beam, tgt_lang_id=lang_id)\n    return ids_to_text(out_ids)\nwith gr.Blocks() as demo:\n    gr.Markdown(\"## Traductor (Transformer desde cero)\")\n    direction = gr.Radio(choices=[\"ncx2es\",\"es2ncx\"], value=\"ncx2es\", label=\"Dirección\")\n    beam = gr.Slider(1,10, step=1, value=5, label=\"Beam size\")\n    inp = gr.Textbox(lines=3, label=\"Texto de entrada\")\n    out = gr.Textbox(lines=3, label=\"Traducción\")\n    btn = gr.Button(\"Traducir\")\n    btn.click(fn=infer_scratch, inputs=[inp, direction, beam], outputs=[out])\nprint(\"Para lanzar la UI: demo.launch()\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Baseline opcional — BERT2BERT (mBERT) con HuggingFace (freezing parcial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 14.1) Dataset HuggingFace\nfrom datasets import Dataset as HFDataset\nfrom transformers import BertTokenizerFast\nHF_DIR = BASE_DIR / \"hf_bert2bert\"; HF_DIR.mkdir(parents=True, exist_ok=True)\ntok_hf = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\ndef build_hf_split(pairs, direction=\"ncx2es\"):\n    srcs, tgts = [], []\n    for s, t, *_ in pairs:\n        if direction==\"ncx2es\": srcs.append(s); tgts.append(t)\n        else: srcs.append(t); tgts.append(s)\n    return HFDataset.from_dict({\"src\":srcs, \"tgt\":tgts})\nhf_train = build_hf_split(train_pairs, \"ncx2es\"); hf_dev = build_hf_split(dev_pairs, \"ncx2es\")\ndef tok_map(batch):\n    model_inputs = tok_hf(batch[\"src\"], truncation=True, max_length=MAX_LEN)\n    with tok_hf.as_target_tokenizer():\n        labels = tok_hf(batch[\"tgt\"], truncation=True, max_length=MAX_LEN)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]; return model_inputs\nhf_train_tok = hf_train.map(tok_map, batched=True, remove_columns=[\"src\",\"tgt\"])\nhf_dev_tok   = hf_dev.map(tok_map,   batched=True, remove_columns=[\"src\",\"tgt\"])\nprint(hf_train_tok)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 14.2) Entrenamiento BERT2BERT (opcional)\nfrom transformers import EncoderDecoderModel, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nenc_dec = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-multilingual-cased\", \"bert-base-multilingual-cased\")\nfor name, param in enc_dec.named_parameters():\n    if \"encoder.embeddings\" in name or \"encoder.encoder.layer.0\" in name or \"decoder.embeddings\" in name:\n        param.requires_grad = False\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tok_hf, model=enc_dec)\nargs = TrainingArguments(output_dir=str(HF_DIR), per_device_train_batch_size=4, per_device_eval_batch_size=4,\n                         evaluation_strategy=\"epoch\", learning_rate=5e-5, num_train_epochs=1,\n                         save_total_limit=1, logging_steps=50, report_to=\"none\")\ntrainer = Trainer(model=enc_dec, args=args, data_collator=data_collator, tokenizer=tok_hf,\n                  train_dataset=hf_train_tok, eval_dataset=hf_dev_tok)\n# trainer.train()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 14.3) Inferencia con mBERT (opcional)\ndef infer_hf(text, max_new_tokens=64):\n    if not text.strip(): return \"\"\n    enc_dec.eval()\n    inputs = tok_hf(text, return_tensors=\"pt\")\n    outputs = enc_dec.generate(**inputs, max_new_tokens=max_new_tokens)\n    return tok_hf.decode(outputs[0], skip_special_tokens=True)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 15) Reanudar entrenamiento desde un checkpoint (ejemplo)\n# ckpt = CHECK_DIR / \"scratch_ncx2es_best.pt\"\n# best_path, best_chrf = resume_direction(str(ckpt), more_epochs=2, patience=3)\n# print(\"Nuevo mejor checkpoint:\", best_path, \" | chrF++:\", best_chrf)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}