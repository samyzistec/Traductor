{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer desde cero para Traducción Náhuatl (ncx) ↔ Español (es)\n\n**Autor:** preparado para Samuel Pérez Zistecatl  \n**Stack:** PyTorch puro (modelo desde cero) + baseline opcional **BERT2BERT** (mBERT, HuggingFace)  \n\nEste notebook cubre: preprocesamiento (SentencePiece Unigram, vocab compartido), dataset, modelo Transformer *from scratch* con **Pre-Norm**, *Noam* LR, **label smoothing**, métricas **sacreBLEU**/**chrF++**, *beam search* y **UI con Gradio** para inferencia.  \n\n> ⚠️ **Uso responsable de datos**: Este notebook asume que ya construiste el corpus `parallel_ncx_es.jsonl` desde fuentes permitidas (p. ej., JW.org) y que respetas sus términos de uso.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Dependencias\nEjecuta esta celda si necesitas instalar paquetes en tu entorno local.\n```bash\n%pip install sentencepiece sacrebleu gradio tqdm pyyaml\n# Opcional (baseline BERT2BERT / mBART):\n%pip install transformers accelerate datasets\n# Opcional (Windows con GPU AMD via DirectML):\n%pip install torch-directml\n# Opcional (spaCy español para segmentar oraciones mejor):\n# %pip install spacy && python -m spacy download es_core_news_sm\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 1) Rutas, semillas y splits\nfrom pathlib import Path\nimport os, random, numpy as np, torch\n\n# Carpeta de trabajo / salida (Windows: usa raw string o /)\nBASE_DIR = Path(r\"C:\\Users\\Samuel Perez\\Desktop\\articulo\")\nBASE_DIR.mkdir(parents=True, exist_ok=True)\n\nDATA_DIR = BASE_DIR / \"salida\"              # aquí debe estar 'parallel_ncx_es.jsonl'\nCHECK_DIR = BASE_DIR / \"checkpoints\"        # pesos del modelo\nTOK_DIR = BASE_DIR / \"spm\"                  # modelos de SentencePiece\nLOG_DIR = BASE_DIR / \"logs\"\n\nfor p in [DATA_DIR, CHECK_DIR, TOK_DIR, LOG_DIR]:\n    p.mkdir(parents=True, exist_ok=True)\n\n# Archivo paralelo\nPARALLEL_JSONL = DATA_DIR / \"parallel_ncx_es.jsonl\"\nassert PARALLEL_JSONL.exists(), f\"No se encontró {PARALLEL_JSONL}. Ajusta DATA_DIR.\"\n\n# Semillas y splits\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\nSPLIT_TRAIN = 0.8\nSPLIT_DEV   = 0.1   # test será 1 - train - dev\nMAX_SAMPLES = 0     # 0 = usar todos; usa p.ej. 20000 para prueba rápida\n\n# Longitud máxima de secuencia\nMAX_LEN = 128\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 2) Dispositivo: CPU por defecto; DirectML opcional si está instalado (GPU AMD).\nimport torch\nDEVICE = torch.device(\"cpu\")\ntry:\n    import torch_directml\n    DEVICE = torch_directml.device()\n    print(\"Usando DirectML (GPU AMD) si está disponible.\")\nexcept Exception as e:\n    print(\"DirectML no disponible; usando CPU.\\n\", str(e))\n\nprint(\"DEVICE =\", DEVICE)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 3) Carga de datos y segmentación por oraciones\nimport json, re\nfrom typing import List\n\ndef sent_split_es(text: str) -> List[str]:\n    \"\"\"Segmentación básica para español; intenta spaCy si está instalado.\"\"\"\n    try:\n        import spacy\n        try:\n            nlp = spacy.load(\"es_core_news_sm\")\n        except Exception:\n            nlp = spacy.blank(\"es\")\n            nlp.add_pipe(\"sentencizer\")\n        return [s.text.strip() for s in nlp(text).sents if s.text.strip()]\n    except Exception:\n        # Fallback regex simple\n        parts = re.split(r\"(?<=[\\.\\?\\!¡¿])\\s+\", text.strip())\n        return [p.strip() for p in parts if p.strip()]\n\ndef sent_split_ncx(text: str) -> List[str]:\n    \"\"\"Segmentación aproximada para náhuatl (reglas por puntuación).\"\"\"\n    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text.strip())\n    return [p.strip() for p in parts if p.strip()]\n\n# Cargar pares del JSONL\npairs = []  # (src, tgt, libro, cap, ver)\nwith open(PARALLEL_JSONL, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        obj = json.loads(line)\n        src = obj[\"src\"].strip()\n        tgt = obj[\"tgt\"].strip()\n        if not src or not tgt:\n            continue\n        pairs.append((src, tgt, obj.get(\"libro\",\"\"), obj.get(\"capitulo\",0), obj.get(\"versiculo\",0)))\n\nprint(f\"Total pares (verso a verso) cargados: {len(pairs):,}\")\n\n# Expandir a nivel oración cuando ambos lados tienen igual # de oraciones; si no, conservar verso.\nexpanded = []\nfor src, tgt, libro, cap, ver in pairs:\n    s_src = sent_split_ncx(src.lower())\n    s_tgt = sent_split_es(tgt.lower())\n    if len(s_src) == len(s_tgt) and 1 < len(s_src) < 10:\n        for i in range(len(s_src)):\n            expanded.append((s_src[i], s_tgt[i], libro, cap, f\"{ver}.{i+1}\"))\n    else:\n        expanded.append((src.lower(), tgt.lower(), libro, cap, ver))\n\nif MAX_SAMPLES and MAX_SAMPLES > 0:\n    expanded = expanded[:MAX_SAMPLES]\n\nprint(f\"Pares después de segmentación-oración (ncx→es) potenciales: {len(expanded):,}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 4) Crear splits train/dev/test (misma partición para ambas direcciones)\nfrom math import floor\nidx = list(range(len(expanded)))\nrandom.shuffle(idx)\n\nn_train = floor(len(idx)*SPLIT_TRAIN)\nn_dev   = floor(len(idx)*SPLIT_DEV)\nn_test  = len(idx) - n_train - n_dev\n\ndef take(idxs): return [expanded[i] for i in idxs]\n\ntrain_pairs = take(idx[:n_train])\ndev_pairs   = take(idx[n_train:n_train+n_dev])\ntest_pairs  = take(idx[n_train+n_dev:])\n\nprint(f\"Train: {len(train_pairs):,} | Dev: {len(dev_pairs):,} | Test: {len(test_pairs):,}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 5) Entrenar SentencePiece (Unigram, vocab compartido)\nimport sentencepiece as spm\n\n# Archivos temporales para entrenamiento\nraw_corpus_path = TOK_DIR / \"spm_raw.txt\"\nwith open(raw_corpus_path, \"w\", encoding=\"utf-8\") as w:\n    for s, t, *_ in train_pairs + dev_pairs:\n        w.write(s + \"\\n\")\n        w.write(t + \"\\n\")\n\nVOCAB_SIZE = 10000  # dentro del rango 8k–12k\nSPM_MODEL_PREFIX = str((TOK_DIR / \"ncx_es_unigram\").as_posix())\n\nspm.SentencePieceTrainer.Train(\n    input=str(raw_corpus_path),\n    model_prefix=SPM_MODEL_PREFIX,\n    vocab_size=VOCAB_SIZE,\n    model_type=\"unigram\",\n    user_defined_symbols=[\"<pad>\",\"<bos>\",\"<eos>\",\"<lang_ncx>\",\"<lang_es>\"],\n    character_coverage=1.0,\n    input_sentence_size=1000000,\n    shuffle_input_sentence=True\n)\n\nSPM_MODEL = TOK_DIR / \"ncx_es_unigram.model\"\nSPM_VOCAB = TOK_DIR / \"ncx_es_unigram.vocab\"\nassert SPM_MODEL.exists(), \"No se generó el modelo SentencePiece.\"\nprint(\"SPM listo:\", SPM_MODEL)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 6) Tokenización utilidades\nsp = spm.SentencePieceProcessor(model_file=str(SPM_MODEL))\n\nPAD_ID   = sp.piece_to_id(\"<pad>\")\nBOS_ID   = sp.piece_to_id(\"<bos>\")\nEOS_ID   = sp.piece_to_id(\"<eos>\")\nLNCX_ID  = sp.piece_to_id(\"<lang_ncx>\")\nLES_ID   = sp.piece_to_id(\"<lang_es>\")\nVOCAB    = sp.get_piece_size()\n\ndef encode_with_lang(text, lang_tok_id):\n    ids = sp.encode(text, out_type=int)\n    ids = [BOS_ID, lang_tok_id] + ids + [EOS_ID]\n    return ids\n\ndef collate_batch(batch, pad_id=PAD_ID):\n    # batch: list of (src_ids, tgt_ids)\n    src_lens = [len(b[0]) for b in batch]\n    tgt_lens = [len(b[1]) for b in batch]\n    max_src = min(max(src_lens), MAX_LEN)\n    max_tgt = min(max(tgt_lens), MAX_LEN)\n\n    def pad_seq(seq, max_len):\n        seq = seq[:max_len]\n        return seq + [pad_id]*(max_len - len(seq))\n\n    import torch\n    src = torch.tensor([pad_seq(b[0], max_src) for b in batch], dtype=torch.long)\n    tgt = torch.tensor([pad_seq(b[1], max_tgt) for b in batch], dtype=torch.long)\n    return src, tgt\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 7) Dataset y bucketing simple\nfrom torch.utils.data import Dataset, DataLoader\nimport random\n\nclass ParallelDataset(Dataset):\n    def __init__(self, pairs, direction=\"ncx2es\"):\n        self.items = []\n        for s, t, *_ in pairs:\n            if direction == \"ncx2es\":\n                src_ids = encode_with_lang(s, LNCX_ID)\n                tgt_ids = encode_with_lang(t, LES_ID)\n            else:\n                src_ids = encode_with_lang(t, LES_ID)\n                tgt_ids = encode_with_lang(s, LNCX_ID)\n            self.items.append((src_ids, tgt_ids))\n\n    def __len__(self): return len(self.items)\n    def __getitem__(self, i): return self.items[i]\n\ndef make_loader(pairs, direction, batch_size=32, shuffle=True):\n    ds = ParallelDataset(pairs, direction=direction)\n    # Bucketing: ordenar por longitud (src) para reducir padding\n    order = sorted(range(len(ds)), key=lambda i: len(ds.items[i][0]))\n    if shuffle:\n        # dividir en cubetas y mezclar entre cubetas\n        B = 50\n        buckets = [order[i::B] for i in range(B)]\n        order = [i for b in buckets for i in random.sample(b, len(b))]\n    class _Proxy(Dataset):\n        def __len__(self): return len(order)\n        def __getitem__(self, j): return ds.items[order[j]]\n    return DataLoader(_Proxy(), batch_size=batch_size, collate_fn=collate_batch)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 8) Transformer desde cero (Pre-Norm)\nimport math\nimport torch.nn as nn\nimport torch\n\nD_MODEL = 512; N_HEADS = 8; DIM_FF  = 2048\nN_LAYERS_ENC = 6; N_LAYERS_DEC = 6; DROPOUT = 0.1\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=2048):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))  # (1, L, D)\n    def forward(self, x):\n        L = x.size(1)\n        return x + self.pe[:, :L]\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.d_k = d_model // n_heads\n        self.n = n_heads\n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.v_proj = nn.Linear(d_model, d_model)\n        self.o_proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, q, k, v, attn_mask=None, key_padding_mask=None):\n        B, Lq, D = q.shape\n        B, Lk, Dk = k.shape\n        q = self.q_proj(q).view(B, Lq, self.n, self.d_k).transpose(1,2)  # (B, h, Lq, d_k)\n        k = self.k_proj(k).view(B, Lk, self.n, self.d_k).transpose(1,2)\n        v = self.v_proj(v).view(B, Lk, self.n, self.d_k).transpose(1,2)\n        scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.d_k)  # (B,h,Lq,Lk)\n\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                scores = scores + attn_mask.unsqueeze(0).unsqueeze(0)\n            elif attn_mask.dim() == 4:\n                scores = scores + attn_mask\n\n        if key_padding_mask is not None:\n            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,Lk)\n            scores = scores.masked_fill(mask, float('-inf'))\n\n        attn = torch.softmax(scores, dim=-1)\n        attn = self.dropout(attn)\n        out = torch.matmul(attn, v)  # (B,h,Lq,d_k)\n        out = out.transpose(1,2).contiguous().view(B, Lq, D)\n        return self.o_proj(out)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n        )\n    def forward(self, x): return self.net(x)\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(d_model)\n        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n        self.drop1 = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.drop2 = nn.Dropout(dropout)\n    def forward(self, x, src_pad_mask):\n        y = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), key_padding_mask=src_pad_mask)\n        x = x + self.drop1(y)\n        y = self.ff(self.norm2(x))\n        x = x + self.drop2(y)\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(d_model)\n        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n        self.drop1 = nn.Dropout(dropout)\n\n        self.norm2 = nn.LayerNorm(d_model)\n        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n        self.drop2 = nn.Dropout(dropout)\n\n        self.norm3 = nn.LayerNorm(d_model)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.drop3 = nn.Dropout(dropout)\n\n    def forward(self, x, mem, tgt_pad_mask, tgt_causal_mask, mem_pad_mask):\n        y = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x),\n                           attn_mask=tgt_causal_mask, key_padding_mask=tgt_pad_mask)\n        x = x + self.drop1(y)\n        y = self.cross_attn(self.norm2(x), mem, mem, key_padding_mask=mem_pad_mask)\n        x = x + self.drop2(y)\n        y = self.ff(self.norm3(x))\n        x = x + self.drop3(y)\n        return x\n\nclass TransformerModel(nn.Module):\n    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_enc, n_dec, dropout=0.1, pad_id=0):\n        super().__init__()\n        self.pad_id = pad_id\n        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n        self.pos = PositionalEncoding(d_model)\n        self.encoder = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_enc)])\n        self.decoder = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_dec)])\n        self.proj = nn.Linear(d_model, vocab_size)\n\n    def make_pad_mask(self, seq):\n        return seq.eq(self.pad_id)  # (B, L)\n\n    def make_causal_mask(self, L):\n        mask = torch.triu(torch.ones(L, L, device=self.emb.weight.device), diagonal=1)\n        mask = mask.masked_fill(mask==1, float('-inf'))\n        return mask  # (L, L)\n\n    def encode(self, src):\n        src_pad = self.make_pad_mask(src)\n        x = self.pos(self.emb(src))\n        for layer in self.encoder:\n            x = layer(x, src_pad)\n        return x, src_pad\n\n    def decode(self, tgt, mem, mem_pad):\n        tgt_pad = self.make_pad_mask(tgt)\n        x = self.pos(self.emb(tgt))\n        causal = self.make_causal_mask(tgt.size(1)).unsqueeze(0).unsqueeze(0)  # broadcast\n        for layer in self.decoder:\n            x = layer(x, mem, tgt_pad, causal, mem_pad)\n        return self.proj(x)\n\n    def forward(self, src, tgt_in):\n        mem, src_pad = self.encode(src)\n        logits = self.decode(tgt_in, mem, src_pad)\n        return logits\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 9) Criterio (Label Smoothing) y Noam Scheduler\nimport torch.nn as nn, torch\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.1, ignore_index=0):\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n\n    def forward(self, pred, target):\n        pred = pred.view(-1, pred.size(-1))\n        target = target.reshape(-1)\n        log_probs = torch.log_softmax(pred, dim=-1)\n        nll = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n        smooth = -log_probs.mean(dim=-1)\n        pad_mask = target.eq(self.ignore_index)\n        loss = self.confidence * nll + self.smoothing * smooth\n        return (loss.masked_fill(pad_mask, 0).sum() / torch.clamp((~pad_mask).sum(), min=1))\n\nclass NoamWrapper:\n    def __init__(self, optimizer, d_model, warmup=4000):\n        self.opt = optimizer\n        self.d_model = d_model\n        self.warm = warmup\n        self.step_num = 0\n    def step(self):\n        self.step_num += 1\n        lr = (self.d_model ** -0.5) * min(self.step_num ** -0.5, self.step_num * (self.warm ** -1.5))\n        for pg in self.opt.param_groups:\n            pg['lr'] = lr\n        self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()\n    @property\n    def lr(self): return self.opt.param_groups[0]['lr']\n\n# Métricas (sacreBLEU / chrF++)\ntry:\n    from sacrebleu.metrics import BLEU, CHRF\n    bleu = BLEU(force=True)\n    chrf = CHRF(word_order=2)\nexcept Exception as e:\n    print(\"sacrebleu no disponible:\", e)\n    bleu = chrf = None\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 10) Entrenamiento\nimport torch\n\ndef batch_to_device(b, device):\n    return b[0].to(device), b[1].to(device)\n\ndef evaluate(model, loader, device):\n    model.eval()\n    total = 0.0; n = 0\n    with torch.no_grad():\n        for src, tgt in loader:\n            src, tgt = batch_to_device((src,tgt), device)\n            logits = model(src, tgt[:, :-1])\n            crit = LabelSmoothingLoss(VOCAB, 0.1, PAD_ID)\n            loss = crit(logits, tgt[:,1:])\n            total += loss.item(); n += 1\n    return total / max(n,1)\n\ndef ids_to_text(ids):\n    ids = [i for i in ids if i not in (PAD_ID, BOS_ID)]\n    if ids and ids[-1] == EOS_ID: ids = ids[:-1]\n    return sp.decode(ids)\n\ndef translate_greedy(model, src_ids, max_len=MAX_LEN):\n    model.eval()\n    src = torch.tensor([src_ids], dtype=torch.long, device=model.emb.weight.device)\n    mem, src_pad = model.encode(src)\n    ys = torch.tensor([[BOS_ID, LES_ID]], dtype=torch.long, device=src.device)  # por defecto ncx->es\n    for _ in range(max_len):\n        logits = model.decode(ys, mem, src_pad)\n        nxt = logits[:,-1,:].argmax(dim=-1, keepdim=True)\n        ys = torch.cat([ys, nxt], dim=1)\n        if nxt.item() == EOS_ID: break\n    return ys[0].tolist()\n\ndef train_direction(direction=\"ncx2es\",\n                    epochs=3, batch_size=32, grad_accum=1,\n                    d_model=512, n_heads=8, d_ff=2048, n_enc=6, n_dec=6,\n                    warmup=4000, save_prefix=\"scratch_ncx2es\"):\n    print(f\"\\n=== Entrenando dirección: {direction} ===\")\n    train_loader = make_loader(train_pairs, direction, batch_size=batch_size, shuffle=True)\n    dev_loader   = make_loader(dev_pairs,   direction, batch_size=batch_size, shuffle=False)\n\n    model = TransformerModel(VOCAB, d_model, n_heads, d_ff, n_enc, n_dec, DROPOUT, PAD_ID)\n    model.to(DEVICE)\n\n    opt = torch.optim.Adam(model.parameters(), betas=(0.9,0.98), eps=1e-9)\n    noam = NoamWrapper(opt, d_model, warmup=warmup)\n    crit = LabelSmoothingLoss(VOCAB, 0.1, PAD_ID)\n\n    best_dev = 1e9; best_path = None\n    for ep in range(1, epochs+1):\n        model.train()\n        total = 0.0; n = 0\n        opt.zero_grad()\n        for i, (src, tgt) in enumerate(train_loader, 1):\n            src, tgt = batch_to_device((src,tgt), DEVICE)\n            logits = model(src, tgt[:, :-1])\n            loss = crit(logits, tgt[:, 1:]) / grad_accum\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            if i % grad_accum == 0:\n                noam.step(); noam.zero_grad()\n            total += loss.item()*grad_accum; n += 1\n            if i % 100 == 0:\n                print(f\"ep{ep} step{i} lr={noam.lr:.6f} loss={total/max(n,1):.4f}\")\n\n        dev_loss = evaluate(model, dev_loader, DEVICE)\n        print(f\"[Ep {ep}] dev_loss={dev_loss:.4f}\")\n        if dev_loss < best_dev:\n            best_dev = dev_loss\n            best_path = (CHECK_DIR / f\"{save_prefix}_best.pt\").as_posix()\n            torch.save({\"model\":model.state_dict(),\n                        \"cfg\":{\"d_model\":d_model,\"n_heads\":n_heads,\"d_ff\":d_ff,\n                               \"n_enc\":n_enc,\"n_dec\":n_dec,\"pad_id\":PAD_ID,\"vocab\":VOCAB}},\n                       best_path)\n            print(\"Guardado mejor modelo en\", best_path)\n\n    return best_path\n\nCFG_SMALL = dict(d_model=512, n_heads=8, d_ff=2048, n_enc=6, n_dec=6)\nCFG_LIGHT = dict(d_model=256, n_heads=4, d_ff=1024, n_enc=4, n_dec=4)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 11) Entrenar ambos sentidos (ajusta epochs según tu tiempo)\nEPOCHS = 2          # Recomendado START (CPU/DirectML). Sube a 6–10 si tienes paciencia.\nBATCH  = 32\nACCUM  = 1\nWARMUP = 4000\n\nbest_ncx2es = train_direction(\"ncx2es\", epochs=EPOCHS, batch_size=BATCH, grad_accum=ACCUM, warmup=WARMUP, save_prefix=\"scratch_ncx2es\", **CFG_SMALL)\nbest_es2ncx = train_direction(\"es2ncx\", epochs=EPOCHS, batch_size=BATCH, grad_accum=ACCUM, warmup=WARMUP, save_prefix=\"scratch_es2ncx\", **CFG_SMALL)\n\nprint(\"Mejores checkpoints:\")\nprint(\" ncx→es:\", best_ncx2es)\nprint(\" es→ncx:\", best_es2ncx)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 12) Evaluación en test con greedy (rápido) y métricas sacreBLEU/chrF\nfrom pathlib import Path\n\ndef load_model(path):\n    import torch\n    data = torch.load(path, map_location=\"cpu\")\n    cfg = data[\"cfg\"]\n    model = TransformerModel(cfg[\"vocab\"], cfg[\"d_model\"], cfg[\"n_heads\"], cfg[\"d_ff\"],\n                             cfg[\"n_enc\"], cfg[\"n_dec\"], pad_id=cfg[\"pad_id\"])\n    model.load_state_dict(data[\"model\"])\n    model.to(DEVICE)\n    model.eval()\n    return model\n\ndef ids_to_text(ids):\n    ids = [i for i in ids if i not in (PAD_ID, BOS_ID)]\n    if ids and ids[-1] == EOS_ID: ids = ids[:-1]\n    return sp.decode(ids)\n\ndef eval_direction(best_path, direction=\"ncx2es\", max_samples=200):\n    if best_path is None or not Path(best_path).exists():\n        print(\"Checkpoint no encontrado:\", best_path); return\n    model = load_model(best_path)\n    ds = ParallelDataset(test_pairs, direction=direction)\n    refs = []; hyps = []\n    for i in range(min(len(ds), max_samples)):\n        src_ids, tgt_ids = ds[i]\n        out_ids = translate_greedy(model, src_ids, max_len=MAX_LEN)\n        refs.append([ids_to_text(tgt_ids)])\n        hyps.append(ids_to_text(out_ids))\n    try:\n        from sacrebleu.metrics import BLEU, CHRF\n        print(direction, \"BLEU:\", BLEU(force=True).corpus_score(hyps, list(zip(*refs))))\n        print(direction, \"chrF++:\", CHRF(word_order=2).corpus_score(hyps, list(zip(*refs))))\n    except Exception as e:\n        print(\"sacrebleu no disponible:\", e)\n        for k in range(min(5, len(hyps))):\n            print(\"SRC:\", ids_to_text(ds[k][0]))\n            print(\"HYP:\", hyps[k])\n            print(\"REF:\", refs[k][0][0]); print()\n\neval_direction(best_ncx2es, \"ncx2es\")\neval_direction(best_es2ncx, \"es2ncx\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 13) Beam search y UI con Gradio\nimport gradio as gr\nimport torch\n\ndef translate_beam(model, src_ids, beam=5, lp=0.7, max_len=MAX_LEN, tgt_lang_id=LES_ID):\n    model.eval()\n    device = model.emb.weight.device\n    src = torch.tensor([src_ids], dtype=torch.long, device=device)\n    mem, src_pad = model.encode(src)\n    beams = [([BOS_ID, tgt_lang_id], 0.0)]\n    finished = []\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score in beams:\n            if seq[-1] == EOS_ID:\n                finished.append((seq, score)); continue\n            ys = torch.tensor([seq], dtype=torch.long, device=device)\n            logits = model.decode(ys, mem, src_pad)[:,-1,:].squeeze(0)\n            logp = torch.log_softmax(logits, dim=-1).detach().cpu()\n            topk = torch.topk(logp, beam).indices.tolist()\n            for tok in topk:\n                new_seq = seq + [tok]\n                new_score = score + logp[tok].item()\n                new_beams.append((new_seq, new_score))\n        beams = sorted(new_beams, key=lambda x: x[1]/((len(x[0])**lp)), reverse=True)[:beam]\n        if not beams: break\n    if not finished: finished = beams\n    best = max(finished, key=lambda x: x[1]/((len(x[0])**lp)))\n    return best[0]\n\n# Intentar cargar mejores checkpoints si existen\nBEST_NCX2ES = best_ncx2es if 'best_ncx2es' in globals() else None\nBEST_ES2NCX = best_es2ncx if 'best_es2ncx' in globals() else None\n\ndef load_scratch(direction):\n    path = BEST_NCX2ES if direction==\"ncx2es\" else BEST_ES2NCX\n    if path is None or not Path(path).exists():\n        return None, f\"Checkpoint no encontrado: {path}\"\n    return load_model(path), f\"Cargado: {path}\"\n\ndef infer_scratch(text, direction=\"ncx2es\", beam=5):\n    if not text.strip():\n        return \"\"\n    model, msg = load_scratch(direction)\n    lang_id = LES_ID if direction==\"ncx2es\" else LNCX_ID\n    src_lang = LNCX_ID if direction==\"ncx2es\" else LES_ID\n    src_ids = encode_with_lang(text.lower(), src_lang)\n    out_ids = translate_beam(model, src_ids, beam=beam, tgt_lang_id=lang_id)\n    return ids_to_text(out_ids)\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"## Traductor (Transformer desde cero)\")\n    direction = gr.Radio(choices=[\"ncx2es\",\"es2ncx\"], value=\"ncx2es\", label=\"Dirección\")\n    beam = gr.Slider(1,10, step=1, value=5, label=\"Beam size\")\n    inp = gr.Textbox(lines=3, label=\"Texto de entrada\")\n    out = gr.Textbox(lines=3, label=\"Traducción\")\n    btn = gr.Button(\"Traducir\")\n    btn.click(fn=infer_scratch, inputs=[inp, direction, beam], outputs=[out])\n\nprint(\"Para lanzar la UI en local: demo.launch()\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Baseline opcional: **BERT2BERT** (mBERT) con HuggingFace\nEsta sección usa `transformers` para crear un `EncoderDecoderModel` con `bert-base-multilingual-cased` como encoder y decoder. En CPU será lento; se sugiere **freezing** parcial.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 14.1) Preparación de dataset para HuggingFace\nfrom datasets import Dataset as HFDataset\nfrom transformers import BertTokenizerFast\n\nHF_DIR = BASE_DIR / \"hf_bert2bert\"\nHF_DIR.mkdir(parents=True, exist_ok=True)\n\ntok_hf = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n\ndef build_hf_split(pairs, direction=\"ncx2es\"):\n    srcs, tgts = [], []\n    for s, t, *_ in pairs:\n        if direction==\"ncx2es\": srcs.append(s); tgts.append(t)\n        else: srcs.append(t); tgts.append(s)\n    return HFDataset.from_dict({\"src\":srcs, \"tgt\":tgts})\n\nhf_train = build_hf_split(train_pairs, \"ncx2es\")\nhf_dev   = build_hf_split(dev_pairs,   \"ncx2es\")\n\ndef tok_map(batch):\n    model_inputs = tok_hf(batch[\"src\"], truncation=True, max_length=MAX_LEN)\n    with tok_hf.as_target_tokenizer():\n        labels = tok_hf(batch[\"tgt\"], truncation=True, max_length=MAX_LEN)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nhf_train_tok = hf_train.map(tok_map, batched=True, remove_columns=[\"src\",\"tgt\"])\nhf_dev_tok   = hf_dev.map(tok_map,   batched=True, remove_columns=[\"src\",\"tgt\"])\n\nprint(hf_train_tok)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 14.2) BERT2BERT EncoderDecoderModel (entrenamiento ligero)\nfrom transformers import EncoderDecoderModel, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\nenc_dec = EncoderDecoderModel.from_encoder_decoder_pretrained(\n    \"bert-base-multilingual-cased\", \"bert-base-multilingual-cased\"\n)\n\n# Congelar capas bajas para CPU\nfor name, param in enc_dec.named_parameters():\n    if \"encoder.embeddings\" in name or \"encoder.encoder.layer.0\" in name or \"decoder.embeddings\" in name:\n        param.requires_grad = False\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tok_hf, model=enc_dec)\nargs = TrainingArguments(\n    output_dir=str(HF_DIR),\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    num_train_epochs=1,\n    save_total_limit=1,\n    logging_steps=50,\n    report_to=\"none\"\n)\ntrainer = Trainer(\n    model=enc_dec,\n    args=args,\n    data_collator=data_collator,\n    tokenizer=tok_hf,\n    train_dataset=hf_train_tok,\n    eval_dataset=hf_dev_tok,\n)\n# trainer.train()  # Descomenta para entrenar (CPU puede tardar)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 14.3) Inferencia con BERT2BERT (opcional)\ndef infer_hf(text, max_new_tokens=64):\n    if not text.strip(): return \"\"\n    enc_dec.eval()\n    inputs = tok_hf(text, return_tensors=\"pt\")\n    outputs = enc_dec.generate(**inputs, max_new_tokens=max_new_tokens)\n    return tok_hf.decode(outputs[0], skip_special_tokens=True)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}